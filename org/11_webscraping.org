#+TITLE:WEBSCRAPING WITH PYTHON
#+AUTHOR: Marcus Birkenkrahe
#+SUBTITLE: CSC 109 - Introduction to programming in Python - Summer 2023
#+property: header-args:python :results output :exports both :noweb yes
#+startup: overview hideblocks indent inlineimages
* README

- This is an outline of several webscraping packages:
  1) ~webbrowser~ - to ~open~ web pages
  2) ~requests~ - to download files and web pages
  3) ~bs4~ - to parse HTML source files
  4) ~selenium~ - to launch and control a web browser

- The development follows chapter 12 (pp. 267-299) in Sweigart (2019)
  and the publicly available documentation for the packages:
  1) [[https://docs.python.org/3/library/webbrowser.html][webbrowser (standard library)]]

- There are some good tutorials from the DataCamp blog:
  1) [[https://www.datacamp.com/tutorial/web-scraping-python-nlp][Web Scraping & NLP in Python (DataCamp tutorial)]] (2017).
  2) [[https://www.datacamp.com/tutorial/web-scraping-using-python][Web Scraping using Python (and Beautiful Soup)]] (2018).
  3) [[https://www.datacamp.com/tutorial/making-web-crawlers-scrapy-python][Making Web Crawlers Using Scrapy for Python]] (2019)

- However, web development is a highly volatile field, with many
  different languages, technologies and standards involved, and I
  would not expect the code from older tutorials to work out of the
  box.

- Making it work nevertheless, however, is a great way to learn about
  a package.

* Using ~webbrowser~ to open a URL

- The ~webbrowser~ module provides an interface to displaying web-based
  documents to users.

- You can call the ~open~ function on the URL to open the page:
  #+begin_src python :results silent
    import webbrowser
    url = 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'
    webbrowser.open(url)
    url = 'https://lyon.edu'
    webbrowser.open(url)
    url = 'https://www.python.org'
    webbrowser.open(url)
  #+end_src

- The script ~webbrowser~ can also be used on the command line. Enter
  this in a terminal window:
  #+begin_example sh
    python -m webbrowser -t "https://www.python.org"
  #+end_example

- These will not work in Colab but they work on the terminal or in a
  Python script.

- As long as you have the URL, ~webbrowser~ lets users cut out the step
  of opening the browser. Sample applications (scripts) include:
  1) open all links on a page in separate browser tabs
  2) open the browser to the URL for your local weather
  3) open several social network sites that you regularly check.

* Example: open Google map with an address only

- We create a script that is run on the command line by the shell
  program (~bash~) though it is a Python file.

- The shell passes an address argument to the script where it is
  received as a list of strings ~sys.argv~.

- To turn the list into a single string value ~address~ (a URL for the
  browser), use ~str.join~, then feed the ~address~ to ~webbrowser.open~.

- You find this script in GitHub in ~py/src~ as ~mapit~ (link):
  #+begin_src sh
    #! python3
    # launch map in browser using an address from the command line
    # import pyperclip and use address = pyperclip.paste for clipboard use

    import webbrowser, sys

    # If there is at least one command line argument
    if len(sys.argv) > 1:
       address = ' '.join(sys.argv[1:])

       # Open the web browser with the constructed URL
       webbrowser.open('https://www.google.com/maps/place/' + address)

       # Write sys.argv to a file and print to the screen
       filename = "address.txt"
       with open(filename, "w") as file:
       print("Contents of sys.argv:")
       for arg in sys.argv:
                  # Write each argument to the file
                  file.write(arg + "\n")
                  # Print confirmation message
                  print(f"sys.argv has been written to {filename}")
  #+end_src

- Download it, open a terminal and run it with an address like this:
  #+begin_src sh
    ./mapit 1014 E Main St, Batesville, AR 72501
  #+end_src

- This will open Google maps to the address and save the list values
  to a file ~address.txt~ which you can view with the command ~cat~.

* Savings

This is what getting a map with or without Python has cost you:
| MANUALLY                     | PYTHON            |
|------------------------------+-------------------|
| Highlight address            | Highlight address |
| Copy address                 | Copy address      |
| Open web browser             | Run ~mapit~         |
| Open ~maps.google.com~         |                   |
| Click the address text field |                   |
| Paste the address            |                   |
| Press enter                  |                   |

* Using ~requests~ to download files from the web

- With ~requests~, you can download files without having to worry about
  network errors, connection problems or data compression.

- This is the equivalent of the ~wget~ Unix command (similar to ~curl~,
  which supports a wide variety of protocols not just HTTP and FTP)

- This package is not part of the standard Python library and must be
  installed (not on Colab or DataCamp):
  #+begin_src sh
    pip install --user requests  # installs for current user only
  #+end_src

  #+RESULTS:
  | Requirement | already | satisfied: | requests                 | in | c:\users\birkenkrahe\appdata\local\programs\python\python311\lib\site-packages | (2.29.0) |           |             |
  | Requirement | already | satisfied: | charset-normalizer<4,>=2 | in | c:\users\birkenkrahe\appdata\local\programs\python\python311\lib\site-packages | (from    | requests) | (3.1.0)     |
  | Requirement | already | satisfied: | idna<4,>=2.5             | in | c:\users\birkenkrahe\appdata\local\programs\python\python311\lib\site-packages | (from    | requests) | (3.4)       |
  | Requirement | already | satisfied: | urllib3<1.27,>=1.21.1    | in | c:\users\birkenkrahe\appdata\local\programs\python\python311\lib\site-packages | (from    | requests) | (1.26.15)   |
  | Requirement | already | satisfied: | certifi>=2017.4.17       | in | c:\users\birkenkrahe\appdata\local\programs\python\python311\lib\site-packages | (from    | requests) | (2022.12.7) |

- Test that ~requests~ installed alright:
  #+begin_src python :results silent
    import requests
  #+end_src

* Download a web page with ~requests.get~

- The ~requests.get~ function takes a string of a URL to download:
  #+name: url
  #+begin_src python
    # a CSV file: gapminder dataset
    url1 = 'https://raw.githubusercontent.com/birkenkrahe/py/main/data/gapminder.csv'
    # a TXT file: Henry James, The American
    url2 = 'https://www.gutenberg.org/files/177/177-0.txt'
  #+end_src
  #+name: res
  #+begin_src python :results silent
    <<url>>
    import requests
    res1 = requests.get(url1)
    res2 = requests.get(url2)
  #+end_src

- Check out the ~type~ of the return value of this function. Remember
  that to check the return value, you need to save the function call
  itself in a variable and print it:
  #+begin_src python
    <<res>>
    print(type(res1))
  #+end_src

- Before reaching out to the file, let's check if the page exists -
  ~requests.status.codes~ contains HTTP status codes:
  #+begin_src python
    <<res>>
    print(f'Page exists: {res1.status_code == requests.codes.ok:}')
  #+end_src

- Look at the (standardized) list of status codes: you'll see 200 for
  "OK", 404 for "not found" etc. ([[https://en.wikipedia.org/wiki/List_of_HTTP_status_codes][here is the complete list]]):
  #+begin_src python
    import requests
    print(help(requests.status_codes))
  #+end_src

- Print the number of characters of the targeted web page, which is
  now stored as one long string:
  #+begin_src python
    <<res>>
    print(len(res1.text))
    print(len(res2.text))
  #+end_src

  #+RESULTS:
  : 7862
  : 794196

- Strings are sequence data (indexed), so we can look at the top of
  the text files like this:
  #+begin_src python
    <<res>>
    print(res1.text[:250])
    print(---------------)
    print(res2.text[:250])
  #+end_src

- Microsoft Windows (inside Emacs) renders the text file (not the CSV)
  with additional control characters. On the Python console, and in
  Colab, it looks worse:
  #+attr_latex: :width 400px
  [[../img/american.png]]

- Connection issues are rampant. Another way to check if the download
  succeeded is to call ~raise_for_status~ on the ~response~ object: if
  there was an error, then an exception will be raised.

- Raise a 404 exception with a non-existent page (incomplete name):
  #+name: bad_url
  #+begin_src python :results silent
    import requests
    bad_url = 'https://www.gutenberg.org/files/177/177'
    res = requests.get(bad_url)
    res.raise_for_status()
  #+end_src

- You can wrap the ~raise_for_status()~ line with ~try...except~ to handle
  the exception:
  #+begin_src python :results silent
    import requests
    bad_url = 'https://www.gutenberg.org/files/177/177'
    res = requests.get(bad_url)
    try:
        res.raise_for_status()
    except Exception as exc:
        print(f'There was a problem: {exc}')
  #+end_src

- Always call ~raise_for_status()~ after calling ~requests.get()~ before
  continuing.

* Save downloaded files

- To save the file from the ~response~ object in Python to a file, use
  the standard library functions ~open~ and ~write~:
  1) ~open~ the file in ~write binary~ mode (parameter ~'wb'~) to maintain
     the Unicode encoding of the text.
  2) ~write~ the web page to a file using ~requests.Response.iter_content~:
  #+begin_src python :results silent
    import requests
    url2 = 'https://www.gutenberg.org/files/177/177-0.txt'
    res = requests.get(url2)
    try:
        res.raise_for_status()
    except Exception as exc:
        print(f'There was a problem: {exc}')

    # open file in write binary mode
    jamesFile = open('TheAmerican.txt','wb')

    # write the web page to file
    for chunk in res.iter_content(100000):
        jamesFile.write(chunk, 'wb')
        bytes_written = jamesFile.write(chunk)
        print(f'Written {bytes_written} bytes')

    # close the output stream to file
    jamesFile.close()
  #+end_src

- The ~iter_content~ method returns chunks of the content on each
  iteration. The chunks are of the ~bytes~ data type and the argument
  specifies how many bytes a chunk can contain (100kB). This prevents
  loading the entire file into memory at once. The ~close()~ function
  flushes all data to disk and frees resources.

- The ~requests~ module contains many more methods for users:
  #+begin_src python
    import requests
    print(len(dir(requests)))
    print(dir(requests))
  #+end_src

* HTML

- HTML (HyperText Markup Language) files are plaintext ~.html~ files

- Though tempting to the initiated, you cannot parse HTML using
  regular expressions ([[https://i.imgur.com/gOPS2.png][see here]]) (which is why I left regex out).

- Text in an HTML file is surrounded by /tags/, which are enclosed in
  angle brackets:
  #+begin_example
    <strong>Hello</strong>, world!
  #+end_example

- You can open this code from within an HTML file with the ~webbrowser~
  module - run this in IDLE as a file ~html.py~:
  #+begin_src python :results silent
    import webbrowser

    # HTML content: whitespace is irrelevant here
    html_content = '<strong>Hello</strong>, world!'

    # Write HTML content to a file
    with open('hello.html', 'w') as file:
        file.write(html_content)

    # Open the file in the web browser
    webbrowser.open('hello.html')
  #+end_src

- Many tags have attributes within the angle brackets. For example,
  open this page to an article on ~aeaweb.org~,
  [[https://www.aeaweb.org/articles?id=10.1257/jel.20201482][aeaweb.org/articles?id=10.1257/jel.20201482]], right-click and select
  ~view page source~ (CTRL + u): after some HTML comments (~<!-- ... -->~)
  follows the ~<html>~ tag, which brackets the entire page: this tag has
  a language attribute ~lang='en'~.

- But you can see that most of the meta information about this paper
  is contained within a page of ~<meta>~ tags with the attribute ~name~.

- To see even more hidden information, you can right click and select
  ~Inspect~ (or open the ~More tools > Developer tools~ browser menu).

* Viewing HTML/CSS source: weather data

- Why would you look at the developer tools?

- Let's say you want to pull weather forecast data from
  https://weather.gov/.

- Enter the Batesville ZIP code ~72501~ in the search field at the top.
  #+attr_latex: :width 400px
  [[../img/weather1.png]]

- Open the ~Inspect~ panel and after some searching, you'll find that
  the current weather conditions for example are included in one ~<div>~
  block:
  #+attr_latex: :width 400px
  [[../img/weather.png]]

- You can copy any element with right-click and selecting ~Copy > Copy
  Element~, and later use this information for scraping:
  #+begin_example
  <div id="current_conditions-summary" class="pull-left">
            ...
            <p class="myforecast-current">Fair</p>
            <p class="myforecast-current-lrg">78°F</p>
            <p class="myforecast-current-sm">26°C</p>
        </div>
  #+end_example

* IN PROGRESS Parsing HTML with the ~bs4~ module

- 'Beautiful Soup' is a module for extracting information from an HTML
  page. The module's real name is ~bs4~ (version 4).

- To install (if not in Colab or DataCamp, or on Python 3.11 which
  comes with Beautiful Soup):
  #+begin_example sh
    pip install --user beautifulsoup4
  #+end_example

- Import the module (there should be no complaints):
  #+begin_src python :results silent
    import bs4
  #+end_src

- For our example, we'll use ~bs4~ to parse (i.e. analyze + identify the
  parts of) a simple HTML file on the hard drive:
  #+begin_example
  <!-- This is the example.html file. -->
  <html><head><title>The Website Title</title></head>
  <body>
    <p>Download the book <strong>The American</strong> from
    <a href="https://www.gutenberg.org/files/177/177-0.txt">Project Gutenberg</a>.</p>
    <p class="slogan">Read more 19th century fiction!</p>
    <p>By <span id="author">Henry James</span></p>
  </body></html>
  #+end_example

- Practice! Create and download the file yourself:
  1) Use ~webbrowser~ to render the file in your browser as ~james.html~
  2) Use ~requests~ to get the file and save it as ~james.html~

* Solution:

1) Use ~webbrowser~ to render the file in your browser as ~example.html~:
   #+begin_src python :results silent
     import webbrowser

     # HTML content - whitespace is irrelevant
     html_content = '  <!-- This is the example.html file. --> <html><head><title>The Website Title</title></head><body> <p>Download the book <strong>The American</strong> from <a href="https://www.gutenberg.org/files/177/177-0.txt">Project Gutenberg</a>.</p><p class="slogan">Read more 19th century fiction!</p> <p>By <span id="author">Henry James</span></p></body></html>'

     # Write HTML content to a file
     with open('example.html', 'w') as file:
         file.write(html_content)

     # Open the file in the web browser
     webbrowser.open('example.html')
   #+end_src

2) Use ~requests~ to get the file from here and save it as ~example.html~:
   #+begin_src python
     url='file:///C:/Users/birkenkrahe/Documents/GitHub/py/org/example.html'
   #+end_src



* TODO Example: opening all search results
* TODO Example: downloading all xkcd cartoons
* TODO Controlling the browser with ~selenium~
* References

- Sweigart, A. (2019). Automate the Boring Stuff with
  Python. NoStarch. URL: [[https://automatetheboringstuff.com/2e/chapter2/][automatetheboringstuff.com]]
- Van Rossum, G., Drake, F. L. (2009). Python 3 Reference Manual. URL:
  https://docs.python.org/3/reference/.

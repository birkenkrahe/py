{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "WEBSCRAPING WITH PYTHON\n=======================\n\n**Author:** Marcus Birkenkrahe\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## README\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   This is an outline of several webscraping packages:\n    1.  `webbrowser` - to `open` web pages\n    2.  `requests` - to download files and web pages\n    3.  `bs4` - to parse HTML source files\n    4.  `selenium` - to launch and control a web browser\n\n-   The development follows chapter 12 (pp. 267-299) in Sweigart (2019)\n    and the publicly available documentation for the packages:\n    1.  [webbrowser (standard library)](https://docs.python.org/3/library/webbrowser.html)\n\n-   There are some good tutorials from the DataCamp blog:\n    1.  [Web Scraping & NLP in Python (DataCamp tutorial)](https://www.datacamp.com/tutorial/web-scraping-python-nlp) (2017).\n    2.  [Web Scraping using Python (and Beautiful Soup)](https://www.datacamp.com/tutorial/web-scraping-using-python) (2018).\n    3.  [Making Web Crawlers Using Scrapy for Python](https://www.datacamp.com/tutorial/making-web-crawlers-scrapy-python) (2019)\n\n-   However, web development is a highly volatile field, with many\n    different languages, technologies and standards involved, and I\n    would not expect the code from older tutorials to work out of the\n    box.\n\n-   Making it work nevertheless, however, is a great way to learn about\n    a package.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using `webbrowser` to open a URL\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   The `webbrowser` module provides an interface to displaying web-based\n    documents to users.\n\n-   You can call the `open` function on the URL to open the page:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import webbrowser\nurl = 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'\nwebbrowser.open(url)\nurl = 'https://lyon.edu'\nwebbrowser.open(url)\nurl = 'https://www.python.org'\nwebbrowser.open(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   The script `webbrowser` can also be used on the command line. Enter\n    this in a terminal window:\n    \n        python -m webbrowser -t \"https://www.python.org\"\n\n-   These will not work in Colab but they work on the terminal or in a\n    Python script.\n\n-   As long as you have the URL, `webbrowser` lets users cut out the step\n    of opening the browser. Sample applications (scripts) include:\n    1.  open all links on a page in separate browser tabs\n    2.  open the browser to the URL for your local weather\n    3.  open several social network sites that you regularly check.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: open Google map with an address only\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   We create a script that is run on the command line by the shell\n    program (`bash`) though it is a Python file.\n\n-   The shell passes an address argument to the script where it is\n    received as a list of strings `sys.argv`.\n\n-   To turn the list into a single string value `address` (a URL for the\n    browser), use `str.join`, then feed the `address` to `webbrowser.open`.\n\n-   You find this script in GitHub in `py/src` as `mapit` (link):\n    \n        #! python3\n        # launch map in browser using an address from the command line\n        # import pyperclip and use address = pyperclip.paste for clipboard use\n        \n        import webbrowser, sys\n        \n        # If there is at least one command line argument\n        if len(sys.argv) > 1:\n           address = ' '.join(sys.argv[1:]) # ' ' is inserted between args\n        \n           # Open the web browser with the constructed URL\n           webbrowser.open('https://www.google.com/maps/place/' + address)\n        \n           # Write sys.argv to a file and print to the screen\n           filename = \"address.txt\"\n           with open(filename, \"w\") as file:\n           print(\"Contents of sys.argv:\")\n           for arg in sys.argv:\n                      # Write each argument to the file\n                      file.write(arg + \"\\n\")\n                      # Print confirmation message\n                      print(f\"sys.argv has been written to {filename}\")\n\n-   Download it, open a terminal and run it with an address like this:\n    \n        ./mapit 1014 E Main St, Batesville, AR 72501\n\n-   This will open Google maps to the address and save the list values\n    to a file `address.txt` which you can view with the command `cat`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Savings!\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is what getting a map with or without Python has cost you:\n\n| MANUALLY|PYTHON|\n|---|---|\n| Highlight address|Highlight address|\n| Copy address|Copy address|\n| Open web browser|Run <code>mapit</code>|\n| Open <code>maps.google.com</code>||\n| Click the address text field||\n| Paste the address||\n| Press enter||\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using `requests` to download files from the web\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   With `requests`, you can download files without having to worry about\n    network errors, connection problems or data compression.\n\n-   This is the equivalent of the `wget` Unix command (similar to `curl`,\n    which supports a wide variety of protocols not just HTTP and FTP)\n\n-   This package is not part of the standard Python library and must be\n    installed (not on Colab or DataCamp):\n    \n        pip install --user requests  # installs for current user only\n\n-   Test that `requests` installed alright:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download a web page with `requests.get`\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   The `requests.get` function takes a string of a URL to download:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a CSV file: gapminder dataset\nurl1 = 'https://raw.githubusercontent.com/birkenkrahe/py/main/data/gapminder.csv'\n# a TXT file: Henry James, The American\nurl2 = 'https://www.gutenberg.org/files/177/177-0.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "<<url>>\nimport requests\nres1 = requests.get(url1)\nres2 = requests.get(url2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Check out the `type` of the return value of this function. Remember\n    that to check the return value, you need to save the function call\n    itself in a variable and print it:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "<<res>>\nprint(type(res1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Before reaching out to the file, let's check if the page exists -\n    `requests.status.codes` contains HTTP status codes:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "<<res>>\nprint(f'Page exists: {res1.status_code == requests.codes.ok:}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Look at the (standardized) list of status codes: you'll see 200 for\n    \"OK\", 404 for \"not found\" etc. ([here is the complete list](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\nprint(help(requests.status_codes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Print the number of characters of the targeted web page, which is\n    now stored as one long string:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ": 7862\n  : 794196"
          ]
        }
      ],
      "source": [
        "<<res>>\nprint(len(res1.text))\nprint(len(res2.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Strings are sequence data (indexed), so we can look at the top of\n    the text files like this:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "<<res>>\nprint(res1.text[:250])\nprint(---------------)\nprint(res2.text[:250])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Microsoft Windows (inside Emacs) renders the text file (not the CSV)\n    with additional control characters. On the Python console, and in\n    Colab, it looks worse:\n    \n    ![img](../img/american.png)\n\n-   Connection issues are rampant. Another way to check if the download\n    succeeded is to call `raise_for_status` on the `response` object: if\n    there was an error, then an exception will be raised.\n\n-   Raise a 404 exception with a non-existent page (incomplete name):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\nbad_url = 'https://www.gutenberg.org/files/177/177'\nres = requests.get(bad_url)\nres.raise_for_status()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   You can wrap the `raise_for_status()` line with `try...except` to handle\n    the exception:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\nbad_url = 'https://www.gutenberg.org/files/177/177'\nres = requests.get(bad_url)\ntry:\n    res.raise_for_status()\nexcept Exception as exc:\n    print(f'There was a problem: {exc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Always call `raise_for_status()` after calling `requests.get()` before\n    continuing.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save downloaded files\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   To save the file from the `response` object in Python to a file, use\n    the standard library functions `open` and `write`:\n    1.  `open` the file in `write binary` mode (parameter `'wb'`) to maintain\n        the Unicode encoding of the text.\n    2.  `write` the web page to a file using `requestsN.Response.iter_content`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\nurl2 = 'https://www.gutenberg.org/files/177/177-0.txt'\nres = requests.get(url2)\ntry:\n    res.raise_for_status()\nexcept Exception as exc:\n    print(f'There was a problem: {exc}')\n\n# open file in write binary mode\njamesFile = open('TheAmerican.txt','wb')\n\n# write the web page to file\nfor chunk in res.iter_content(100000):\n    jamesFile.write(chunk)\n    bytes_written = jamesFile.write(chunk)\n    print(f'Written {bytes_written} bytes')\n\n# close the output stream to file\njamesFile.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   The `iter_content` method returns chunks of the content on each\n    iteration. The chunks are of the `bytes` data type and the argument\n    specifies how many bytes a chunk can contain (100kB). This prevents\n    loading the entire file into memory at once. The `close()` function\n    flushes all data to disk and frees resources.\n\n-   The `requests` module contains many more methods for users:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\nprint(len(dir(requests)))\nprint(dir(requests))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HTML code and CSS classes\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   HTML (HyperText Markup Language) files are plaintext `.html` files\n\n-   Though tempting to the initiated, you cannot parse HTML using\n    regular expressions ([see here](https://i.imgur.com/gOPS2.png)) (which is why I left regex out).\n\n-   Text in an HTML file is surrounded by *tags*, which are enclosed in\n    angle brackets:\n    \n        <strong>Hello</strong>, world!\n\n-   CSS (Cascading Style Sheets) are essentially functions to change the\n    layout without having to change every single HTML file. Example:\n    \n        <div class=\"container\">\n            <p class=\"text\">Hello</p>\n            <p id=\"special\">World</p>\n        </div>\n        <p class=\"text\">Outside</p>\n\n-   This code contains:\n    1.  one `div` divider element\n    2.  two CSS classes (aka functions), `.container` and `.text`\n    3.  three `p` elements (new paragraph)\n    4.  one `id` attribute with the value `special` inside a `p` element\n        (attributes can be linked to like `#special` or `?id=special`)\n\n-   You can open this code from within an HTML file with the `webbrowser`\n    module - run this in IDLE as a file `html.py`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import webbrowser\n\n# HTML content: whitespace is irrelevant here\nhtml_content = '<strong>Hello</strong>, world!'\n\n# Write HTML content to a file\nwith open('hello.html', 'w') as file:\n    file.write(html_content)\n\n# Open the file in the web browser\nwebbrowser.open('hello.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Many tags have attributes within the angle brackets. For example,\n    open this page to an article on `aeaweb.org`,\n    [aeaweb.org/articles?id=10.1257/jel.20201482](https://www.aeaweb.org/articles?id=10.1257/jel.20201482), right-click and select\n    `view page source` (CTRL + u): after some HTML comments (`<!-- ... -->`)\n    follows the `<html>` tag, which brackets the entire page: this tag has\n    a language attribute `lang='en'`.\n\n-   But you can see that most of the meta information about this paper\n    is contained within a page of `<meta>` tags with the attribute `name`.\n\n-   To see even more hidden information, you can right click and select\n    `Inspect` (or open the `More tools > Developer tools` browser menu).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Viewing HTML/CSS source: weather data\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Why would you look at the developer tools?\n\n-   Let's say you want to pull weather forecast data from\n    [https://weather.gov/](https://weather.gov/).\n\n-   Enter the Batesville ZIP code `72501` in the search field at the top.\n    \n    ![img](../img/weather1.png)\n\n-   Open the `Inspect` panel and after some searching, you'll find that\n    the current weather conditions for example are included in one `<div>`\n    block:\n    \n    ![img](../img/weather.png)\n\n-   You can also right click and `inspect` any element and the inspector\n    will jump to the respective element:\n    \n    ![img](../img/weather1.png)\n\n-   You can copy any element with right-click and selecting `Copy > Copy\n      Element`, and later use this information for scraping:\n    \n        <div id=\"current_conditions-summary\" class=\"pull-left\">\n                  ...\n                  <p class=\"myforecast-current\">Fair</p>\n                  <p class=\"myforecast-current-lrg\">78°F</p>\n                  <p class=\"myforecast-current-sm\">26°C</p>\n              </div>\n\n-   You can also `Copy selector` directly, which will usually result in a\n    much longer string which you may have to analyze to get the element\n    and attribute or attribute value that you really want.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parsing HTML with the `bs4` module\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   'Beautiful Soup' is a module for extracting information from an HTML\n    page. The module's real name is `bs4` (version 4).\n\n-   To install (if not in Colab or DataCamp, or on Python 3.11 which\n    comes with Beautiful Soup):\n    \n        pip install --user beautifulsoup4\n\n-   Import the module (there should be no complaints):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import bs4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   For our example, we'll use `bs4` to parse (i.e. analyze + identify the\n    parts of) a simple HTML file on the hard drive. Get this file from\n    GitHub: [bit.ly/beautifulBook](https://bit.ly/beautifulBook).\n    \n        <!-- This is the example.html file. -->\n        <html><head><title>The Website Title</title></head>\n        <body>\n          <p>Download the book <strong>The American</strong> from\n          <a href=\"https://www.gutenberg.org/files/177/177-0.txt\">Project Gutenberg</a>.</p>\n          <p class=\"slogan\">Read more 19th century fiction!</p>\n          <p>By <span id=\"author\">Henry James</span></p>\n        </body></html>\n\n-   Use `webbrowser` to render the file in your browser as `example.html`\n    (you'll have to do this on a Python console other than Colab,\n    e.g. IDLE):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ": True\n  : c:\\Users\\birkenkrahe\\Documents\\GitHub\\py\\org"
          ]
        }
      ],
      "source": [
        "import webbrowser\n\n# HTML content - whitespace is irrelevant\nhtml_content = '  <!-- This is the example.html file. --> <html><head><title>The Website Title</title></head><body> <p>Download the book <strong>The American</strong> from <a href=\"https://www.gutenberg.org/files/177/177-0.txt\">Project Gutenberg</a>.</p><p class=\"slogan\">Read more 19th century fiction!</p> <p>By <span id=\"author\">Henry James</span></p></body></html>'\n\n# Write HTML content to a file\nwith open('example.html', 'w') as file:\n    file.write(html_content)\n\n# check if the file is there\nimport os\nprint(os.path.isfile('example.html'))\n\n# check where you are - get current working directory\nprint(os.getcwd())\n\n# Open the file in the web browser\nwebbrowser.open('example.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Enter this in IDLE on the interactive shell/console and see if it\n    works.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Beautiful Soup Object from HTML\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   The `bs4.BeautifulSoup` function is called with a string containing\n    the HTML it will parse. It returns a `BeautifulSoup` object (on which\n    various methods will work).\n\n-   Example (you can do this in Colab):\n    1.  get a HTML page\n    2.  raise an status exception check\n    3.  pass response text to bs4.Beautiful Soup\n    4.  show the type of the bs4 object\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests, bs4\n\n# download the main page from Project Gutenberg\nres = requests.get('https://gutenberg.org')\nres.raise_for_status()   # check request status\nres       # prints status\nres.text   # prints downloaded text\n\n# Pass the text attribute of the response to bs4.BeautifulSoup\ngutenbergSoup = bs4.BeautifulSoup(res.text, 'html.parser')\nprint(type(gutenbergSoup))\ngutenbergSoup   # parsed HTML code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Now download `example.html` from here: [bit.ly/beautifulBook](https://bit.ly/beautifulBook) - in\n    Colab, you can upload it to the temporary directory (see sidebar):\n    \n        %cat example.html   # 'magic' command to display the file in Colab\n\n-   Use `requests` to load the HTML file from your hard drive and pass a\n    `File` object instead of a `requests.Response` to `bs4.BeautifulSoup`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests, bs4\n\nexampleFile = open('example.html')  # open file to stdio\nexampleSoup = bs4.BeautifulSoup(exampleFile, 'html.parser')\nprint(type(exampleSoup))\nexampleSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   The `html.parser` comes with Python (there is a faster parser in the\n    `lxml` module - [see here](https://lxml.de/parsing.html)).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finding an element with `select`\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![img](../img/css.png)\n\n-   The `select` function uses CSS selectors to match elements or tags,\n    like classes, IDs etc. It returns a list of elements matching the\n    selector.\n\n-   Try this yourself with this HTML code ([Gist link](https://gist.github.com/birkenkrahe/aae30292c9ee2866342456cf4b906dcb) - bit.ly/3qUy8nb ):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ": container text: Hello\n  : p elements: Hello\n  : p elements: World\n  : p elements: Outside\n  : [<p class=\"text\">Hello</p>, <p id=\"special\">World</p>, <p class=\"text\">Outside</p>]\n  : <class 'bs4.element.ResultSet'>"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n\nhtml = \"\"\"\n<div class=\"container\">\n    <p class=\"text\">Hello</p>\n    <p id=\"special\">World</p>\n</div>\n<p class=\"text\">Outside</p>\n\"\"\"\nsoup = BeautifulSoup(html, 'html.parser')\n\n# Example: all elements that use a CSS `class` attribute named `.container and .text`\nelements = soup.select('.container .text')\nfor element in elements:\n    print(f'container text: {element.text}')\n\nelements = soup.select('p') # look for all paragraph elements\nfor element in elements:\n    print(f'p elements: {element.text}')\n\n# result values are stored in a list (sequence data)\nprint(elements)\n\n# the data type is specific to bs4\nprint(type(elements))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Use this code to select the following elements:\n    1.  Elements in the CSS class 'text'\n    2.  Elements named 'div'\n    3.  Elements named 'p' with 'id' value\n    4.  Elements named 'p' with 'id' value 'special'\n\n-   Solution:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#+begin_example\n  container text: Hello\n  '.text' element: Hello\n  '.text' element: Outside\n  div elements: \n  Hello\n  World\n\n  p elements Hello\n  p elements World\n  p elements Outside\n  p elements with 'id' value: World\n  p elements with 'class=special' value: World\n  #+end_example"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n\nhtml = \"\"\"\n<div class=\"container\">\n    <p class=\"text\">Hello</p>\n    <p id=\"special\">World</p>\n</div>\n<p class=\"text\">Outside</p>\n\"\"\"\nsoup = BeautifulSoup(html, 'html.parser')\n\nelements = soup.select('.container .text') # 'text' in '.container' class\nfor element in elements:\n    print(f'container text: {element.text}')\n\nelements = soup.select('.text') # elements in the '.text' class\nfor element in elements:\n    print(f\"'.text' element: {element.text}\")\n\nelements = soup.select('div')  # elements named 'div'\nfor element in elements:\n    print(f'div elements: {element.text}')\n\nelements = soup.select('p')  # elements named 'p'\nfor element in elements:\n    print(f'p elements {element.text}')\n\nelements = soup.select('p[id]')  # elements named 'p' w/id' value\nfor element in elements:\n    print(f\"p elements with 'id' value: {element.text}\")\n\nelements = soup.select('p[id=\"special\"]')  # p elements with id = 'special'\nfor element in elements:\n    print(f\"p elements with 'class=special' value: {element.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Selector patterns can be combined to make sophisticated matches:\n    this pattern will match any element that has an `id` attribute of\n    `author` as long as it is also inside a `p` element\n    \n        soup.select('p #author') # selects <p id=\"author\">THIS</p>\n\n-   What will this pattern select?\n    \n        soup.select('span .text')\n\n>   Any element of the CSS class 'text' inside a `span` element:\n>   <span class=\"text\">THIS</span>\n\n-   The tag values of the `soup.select` result `list` can be passed to `str`\n    to show the HTML tags they represent, and an `attrs` attribute that\n    shows all HTML attributes of the tag as a `dictionary`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n\nhtml = \"\"\"\n<div class=\"container\">\n    <p class=\"text\">Hello</p>\n    <p id=\"special\">World</p>\n</div>\n<p class=\"text\">Outside</p>\n\"\"\"\nsoup = BeautifulSoup(html, 'html.parser')\n\nelements = soup.select('.text')\nprint(elements[0])\nprint(type(elements[0]))\nprint(type(str(elements[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example HTML file: extract text and attributes\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Here is the example HTML with mostly HTML and one CSS class element:\n    \n        <!-- This is the example.html file. -->\n        <html><head>\n                <title>The Website Title</title>\n              </head>\n              <body>\n                <p>Download the book <strong>The American</strong> from\n                   <a href=\"https://www.gutenberg.org/files/177/177-0.txt\">Project Gutenberg</a>.\n                </p>\n                <p class=\"slogan\">Read more 19th century fiction!</p>\n                <p>By <span id=\"author\">Henry James</span>\n                </p>\n              </body>\n        </html>\n\n-   Pull the element with `id=\"author\"` out of the example HTML:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import bs4\n\n# open the HTML file\nexampleFile = open('example.html')\n\n# parse HTML from file\nexampleSoup = bs4.BeautifulSoup(exampleFile.read(), 'html.parser')\n\n# select all 'id' attributes with the value 'author':\nelements = exampleSoup.select('#author')\n\nprint(isinstance(elements,list))\nprint(elements)     # Output: list\nprint(len(elements))  # Output: 1\nprint(type(elements[0]))  # Output: bs4.element.Tag\n\n# the tag object as a string\nprint(str(elements[0]))\n\n# get the tag text string\nprint(elements[0].getText())\nprint(isinstance(elements[0].getText(),str))\n\n# get the tag attributes dictionary\nprint(elements[0].attrs)\nprint(isinstance(elements[0].attrs,dict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   As an exercise, pull the `<p>` elements from the example HTML and\n    1.  get the text of the 1st list item with `getText()`\n    2.  turn the 2nd list item into a string with `str`\n    3.  get the text of the 2nd list item with `getText()`\n    4.  turn the 3rd list item into a string with `str`\n    5.  get the text of the 3rd list item with `getText()`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import bs4\nexampleFile = open('example.html')\nexampleSoup = bs4.BeautifulSoup(exampleFile.read(), 'html.parser')\n# select all 'p' elements\npElements = exampleSoup.select('p')\n\nfor i in range(3):\n    print(pElements[i].getText())\n    print(str(pElements[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting data from an element's attributes\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   You can use the `get` method to access attribute values from an\n    element: you pass a string of an attribute name, e.g. 'id', and get\n    the value in return:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import bs4\n\n# open the HTML file\nexampleFile = open('example.html')\n\n# parse HTML from file\nexampleSoup = bs4.BeautifulSoup(exampleFile.read(), 'html.parser')\n\n# select the first 'span' element\nspanElement = exampleSoup.select('span')[0]\n\nprint(str(spanElement))\nprint(spanElement.get('id'))\nprint(spanElement.get('some_nonexistent_address') == None)\nprint(spanElement.attrs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Webbrowser\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Python has a built-in `webbrowser`.\n-   You can open URLs with the `webbrowser.open` function.\n-   You can pass command line arguments as a `sys.argv` string\n-   The function `str.join` concatenates strings and inserts `str`\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Downloading web pages and files\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   The Requests module is a third-party module for downloading web\n    pages and files.\n-   `requests.get()` returns a `Response` object.\n-   The `raise_for_status()` Response method will raise an exception if\n    the download failed.\n-   You can save a downloaded file to your hard drive with calls to the\n    `iter_content()` method.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HTML and CSS\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   HTML files are text files that contain HTML and CSS elements and\n    JavaScript (or similar) for dynamic layout creation\n-   For `Response.select` from `requests`, you need to identify CSS\n    selectors like classes, attributes and attribute values\n-   You can inspect the source code of a web page and copy code elements\n    or CSS selectors corresponding to the page elements that interest\n    you.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parsing web pages\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Web pages are plaintext files formatted as HTML.\n-   HTML can be parsed with the BeautifulSoup module.\n-   BeautifulSoup is imported with the name `bs4`.\n-   Pass the string with the HTML to the `bs4.BeautifulSoupx` function to\n    get a `Soup` object.\n-   The `Soup` object has a `select` method that can be passed a string of\n    the CSS selector for an HTML tag.\n-   You can get a CSS selector string from the browser's developer tools\n    by right-clicking the element and selecting `Copy CSS Path`.\n-   The `select` method will return a list of matching Element objects.\n-   The simpler `find` method returns the first occurrence of a specific\n    element, e.g. `soup.find('div')`, so you have to loop to find more.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Project: opening all search results\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   This script accepts a search term on the command line and opens a\n    browser with the top search results in new tabs.\n\n-   The script uses the URL for the Python Package Index at\n    [https://pypi.org>](https://pypi.org>)but you can adapt it to any website.\n\n-   Program flow:\n    \n    ![img](../img/search_bpmn.png \"BPMN model of the program flow for search & open\")\n    \n    1.  Get search keywords from command line arguments.\n    2.  Retrieves the search results page.\n    3.  Opens a browser tab for each result.\n\n-   Python script requirements:\n    \n    ![img](../img/search_full_bpmn.png \"BPMN model of the program flow for searchpypi.py\")\n    \n    1.  Read command line arguments from `sys.argv`.\n    2.  Fetch the result page with the `requests` module.\n    3.  Find the links to each search result with `bs4`.\n    4.  Call `webbrowser.open` to open the web browser.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get cmd line arguments and request search page\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![img](../img/11_pypi_search.png \"search string for a Google search at pypi.org for 'webscraping'\") ![img](../img/11_google_search.png \"search string for a Google search at pypi.org for 'webscraping'\")\n\n-   The result page of a search at [https://pypi.org>](https://pypi.org>)has the URL format:\n\n    https://pypi.org/search/?q=[search_term]\n\n-   We use the trick from `mapit.py` to attach the command line argument\n    to the argument for `requests.get` with `str.join` and `sys.argv` :\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#! python3\n# searchpypi.py - opens several search results\n\n# import required modules\nimport requests, sys, webbrowser, bs4\n\n# download search result page\nprint('Searching...')\nres = requests.get('https://pypi.org/?q='+' '.join(sys.argv[1:]))\nres.raise_for_status()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Remember that `res` holds the return value from `requests.get`, while\n    `res.text` is a string attribute holding the HTML text of that page.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "<<request>>\nprint(res)  # return value from requests.get\nprint(type(res))\nprint(res.text)\nprint(type(res.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   In `'+'.join('ab')`, + is inserted between 'a' and 'b':\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('http://pypi.org/search/?q=' + '+'.join('webscraping'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   When the program is launched, the user specifies the search terms as\n    command line arguments. These arguments will be stored as a list in\n    `sys.argv`.\n\n-   Why do we start with the index 1 in `sys.argv`? Because `sys.argv[0]`\n    stores the name of the list: when you enter\n    \n        python myscript.py arg1  arg2 arg3\n    \n    the list will be:\n    \n        ['myscript.py', 'arg1', 'arg2', 'arg3']\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Find all the results\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   To apply `select` from `Beautiful Soup`, you need to identify the right\n    selector. But using the `<a>` tag element returns all links and not\n    just the search links.\n\n-   You need to open the page inspector and find the pattern that all\n    search results share: it is the CSS class `.package-snippet` in the\n    `<main id=\"content\">` block. The link itself is the value of the\n    `'href'` attribute:\n    \n    ![img](../img/11_pypi_bs4.png)\n\n-   Nice: you don't have to know what this CSS class is or what it\n    does. You don't need to know h\n    ow to write pages like these. You're\n    only going to use it as a marker for the `<a>` element.\n\n-   Add this code to the earlier block:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "<<request>>\n\n# Parse the search page HTML code from the Response object\nsoup = bs4.BeautifulSoup(res.text, 'html.parser')\nprint(soup)\n\n# Select link elements with the CSS class 'package-snippet'\nlinkElements = soup.select('.package-snippet')\nprint(linkElements)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   If the PyPi website changes its layout, you may need to update the\n    script with the appropriate CSS selector string for `soup.select`.\n\n-   What about the content of `linkElements` and the element attributes?\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ": Searching...\n  : []"
          ]
        }
      ],
      "source": [
        "<<soup>>\nprint(str(linkElements))\nprint(linkElements.attrs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   The last code block returns an empty list and raises an\n    `AttributeError` - can you think why?\n\n>   Our search depends on passing a search term as a command line\n>   argument to `requests` - but there's no default search term. We have\n>   not done that so `requests.get` downloads the page without results.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Open web browsers for each result\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Finally, we open web browsers for each result:\n    1.  loop over (at most 5 of) the list `linkElements`.\n    2.  build a URL string `urlToOpen` from the value of the `href` attribute\n        of each link element.\n    3.  feed the URL `urlToOpen` to `webbrowser.open`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "<<soup>>\n# We want at most 5 search results\nnumOpen = min(5, len(linkElements))\n\nfor i in range(numOpen):\n    urlToOpen = 'https://pypi.org' + linkElements[i].get('href')\n    print('Opening', urlToOpen)\n    webbrowser.open(urlToOpen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   The `min` function takes care of the possibility that our search\n    resulted in fewer than 5 links: take 5 or whatever is smaller.\n\n-   The `href` attribute value does not have the full URL (you can only\n    see this after looking at the inspector), which is why we add it.\n\n-   To run the program download `searchpypi.py` from GitHub's `py/src/` repo\n    and run it on a command line like a shell script or with `python`:\n    \n        $ ./searchpypi webscraping  # run as script using the shebang line\n        $ python searchpypi.py webscraping   # run as python program\n\n-   Sample output for command `python searchpypi.py webscraping`\n    addition to 5 windows being opened):\n    \n    ![img](../img/search_output.png)\n\n-   This Colab notebook contains the minimal code (no exception\n    handling): [https://bit.ly/3NExnaC](https://bit.ly/3NExnaC)\n\n-   Here is the full program with exception handling for export (you can\n    download it [from GitHub](https://github.com/birkenkrahe/py/blob/main/src/searchpypi.py) in the `py/src` repository):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#! python3\n# searchpypi.py - opens several search results\n\n# import required modules\nimport requests, sys, webbrowser, bs4\n\n# Check if argument is provided\nif len(sys.argv) < 2:\n    print(\"Please provide a search term\")\n    sys.exit()\n\n# download search result page\nprint('Searching...')\nres = requests.get('https://pypi.org/search/?q=' + '+'.join(sys.argv[1:]), headers={\"User-Agent\": \"Mozilla/5.0\"})\nres.raise_for_status()\n\n# Print the response text for debugging\n#print(res.text)\n\n# Retrieve top search result links\nsoup = bs4.BeautifulSoup(res.text, 'html.parser')\n\n# Replace with the correct CSS selector for search results on PyPI\nlinkElements = soup.select('.package-snippet')  # This might need to be updated\n\n# We want at most 5 search results\nnumOpen = min(5, len(linkElements))\n\nif numOpen == 0:\n    print(\"No results found\")\nelse:\n    for i in range(numOpen):\n        try:\n            href = linkElements[i].get('href')\n            if href:\n                urlToOpen = 'https://pypi.org' + href\n                print('Opening', urlToOpen)\n                webbrowser.open(urlToOpen)\n            else:\n                print(\"No href found for result\", i)\n        except AttributeError as e:\n            print(\"Error processing result\", i, \":\", str(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   A search without search term and with a nonsense search term is\n    covered by this code:\n    \n    ![img](../img/search_output1.png)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ideas for similar programs\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Open all the product pages after searching a shopping site such as\n    Amazon.\n-   Open all the links to reviews for a single product.\n-   Open the result links to photos after performing a search on a photo\n    site such as Flickr or Imgur.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Controlling the browser with `selenium`\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   The `selenium` module controls the browser by clicking links and\n    filling in login information.\n\n-   To do this, a web browser must be launched, so `selenium` is slower\n    and harder to run in the background.\n\n-   You must do this if your interaction with a web page depends on the\n    `<script>` JavaScript code that dynamically updates the page.\n\n-   Commercial sites run security programs to stop people from\n    harvesting their information or creating multiple free\n    accounts. They change often and will break your non-`selenium` Python\n    scripts.\n\n-   Security programs identify you with a *user-agent* string that is\n    served by the web browser and is included in all HTTP requests.\n\n-   For example the user-agent string for Python's `requests` module\n    v.2.29 is `Python-requests/2.29.0`. But for `selenium`, user-agent is\n    the same as a regular browser (see [whatsmyua.info](https://www.whatsmyua.info/)) and it mimicks\n    the browser's traffic patterns (downloads, ads, cookies etc.).\n\n-   Since `selenium` is not entirely undetectable, however, there are\n    sites which do not allow it to scrape their web pages.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Starting a `selenium` controlled browser\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   You can install `selenium` on the command line via `pip`:\n    \n        pip install --user selenium\n\n-   Check installation:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import selenium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extended example: downloading all xkcd cartoons\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   [See bs4's online documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n\n-   Code:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#! python3\n# downloadXkcd.py - Downloads every single XKCD comic.\nimport requests, os, bs4\nurl = 'https://xkcd.com'               # starting url\nos.makedirs('xkcd', exist_ok=True)    # store comics in ./xkcd\nwhile not url.endswith('#'):\n   # Download the page.\n   print('Downloading page %s...' % url)\n   res = requests.get(url)\n   res.raise_for_status()\n   soup = bs4.BeautifulSoup(res.text, 'html.parser')\n# Find the URL of the comic image.\ncomicElem = soup.select('#comic img')\nif comicElem == []:\n   print('Could not find comic image.')\nelse:\n   comicUrl = 'https:' + comicElem[0].get('src')\n# Download the image.\nprint('Downloading image %s...' % (comicUrl))\nres = requests.get(comicUrl)\nres.raise_for_status()\n# Save the image to ./xkcd.\nimageFile = open(os.path.join('xkcd', os.path.basename(comicUrl)),'wb')\nfor chunk in res.iter_content(100000):\n    imageFile.write(chunk)\n    imageFile.close()\n# Get the Prev button's url.\nprevLink = soup.select('a[rel=\"prev\"]')[0]\nurl = 'https://xkcd.com' + prevLink.get('href')\nprint('Done.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Ideas for similar programs:\n    1.  Back up an entire site by following all of its links.\n    2.  Copy all the messages off a web forum.\n    3.  Duplicate the catalog of items for sale on an online store.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Working with JSON APIs\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenize text with `nltk`\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Source: [https://www.datacamp.com/tutorial/web-scraping-python-nlp](https://www.datacamp.com/tutorial/web-scraping-python-nlp)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Sweigart, A. (2019). Automate the Boring Stuff with\n    Python. NoStarch. URL: [automatetheboringstuff.com](https://automatetheboringstuff.com/2e/chapter2/)\n-   Van Rossum, G., Drake, F. L. (2009). Python 3 Reference Manual. URL:\n    [https://docs.python.org/3/reference/](https://docs.python.org/3/reference/).\n\n"
      ]
    }
  ],
  "metadata": [
    [
      "org"
    ],
    null,
    null
  ],
  "nbformat": 4,
  "nbformat_minor": 0
}